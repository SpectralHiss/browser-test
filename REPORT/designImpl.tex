\chapter{Design and implementation}

\section{Improving extensibility and coverage}
\label{label:addtest}

\subsection{Test management interface}

The first thing that was deemed crutial to extend browseraudit was to enable the adding of tests in a convenient manner, this section will
guide you through the components we needed to build this interface and what design choices entail in our implementation.
At this point it is important for understanding this project to review appendix \ref{app:A}. in particular to understand
the structure of the application meaning BrowserAudit V2 according to our appendix's terminology. It is good to have understood
the data model diagram in \ref{fig:model}. 

\subsubsection{Scoping}

At the time of project handover there were no more than 5 top categories and a total of under 40 categories and subcategories.
in light of prioritising time, our first scoping choice was not to pursue the feature that would allow the user to add categories through our interface.

Indeed, our final implementation forces developers to manually add categories, and we had to do this for one new category and subcategory.
we did this manually but in a future proof way, by making the new categories explicit queries in a database migration file (or .sql file) in order to transparently allow setup on new
servers or cross development database servers.

This allows us to focus on the most important part which is the working principle of adding tests to Browseraudit with the least pain possible.

The other scoping choice was regarding what the range of new tests that we could add to our interface. As explained in \ref{label:workprinc}, 
in order for tests to run , go server functions are set-up for each particular kind of category. 
In scoping the domain of tests to be added our choice was to let users add new functions or reuse template functions.

some of the things we did to facilitate this is to add previously mentioned new top level category for what are called "client-only tests"
which only rely on client side functions only. Intended to be hosting any tests that use a new function that doesn't rely on any of the go code
or template functions that don't fit other categories.

We also allow to reuse any of the current client test template functions and provide utilities to help with this meaning
that you can reuse any of the go server functions. Lastly you can also add these new functions to any category if you wish and they will work
as expected, providing a reasonable degree of flexibility.

Don't worry if this is not entirely clear at the moment,  understanding  the domain of tests be easier once you read the
working principle of how test adding works and how our interface works in \ref{label:workprinc}

\subsubsection{What our test interface can do}

Currently, our test interface allows you to choose a particular (sub) category , enter a title , test function , time-out and add the test to the database.
It also has a template function fetcher which fetches the test template function for that category along with example usage from the tests that are already in the database.
A quick reminder of template functions: they are the generic functions that categories use to test a particular function, usually following a particular
endpoint pattern on the server or variations on the same headers, cache directives or different kind of templates fetched etc..

In order to allow this , model wise we had to alter the test function invocation field of our test to be able to hold arbitrarily long Javascript text.
The technical working principle of how tests added in this manner eventually run is explained quite precisely in \ref{subsec:princ}. 
If you have a version of browseraudit going head to the /manage\_test path or from the main screen follow the manage admins page with the following credentials:

\begin{verbatim}
username:crackthecode
password:hello
\end{verbatim}

\subsubsection{Test working principle}
\label{subsec:princ}

So far you've seen that our test interface can add new test functions, let's first show how you would describe a new test function in our interface:
Among the many boxes you are presented in the test management interface is the add test function box which contains a code editing textarea with the following
text already in place:
\begin{verbatim}
function(){
// this function is called with a context containing
// three functions, PASS, WARNING , CRITICAL or SKIP, usage:

	this.WARNING("Reason for the warning");
/*
 alternatively you can make calls to one of the 
 below functions like cspTest(foo, bar ,jah); 
 for any category you can get the template function below
*/} 
\end{verbatim}

This works because this function body that is saved to the database is loaded as the testFunction object in the below text.
Which is invoked with the call method of the Function object , it is called in this manner in order to allow specification
of the "this" context to be the object containing the PASS, WARNING , CRITICAL and SKIP functions that are used above. 

\begin{verbatim}
// Execute the test function, using a "test harness" object as the
		// context (i.e. the "this" object)
		thisNode.testFunction.call({
			PASS: function(reason) { markHierarchyNode(thisNode.nodeType, thisNode.id,
			testStartTime, [ "pass", reason ], testTimer); },
			WARNING: function(reason) { markHierarchyNode(thisNode.nodeType, thisNode.id, 
			testStartTime, [ "warning", reason ], testTimer); },
			CRITICAL: function(reason) { markHierarchyNode(thisNode.nodeType, thisNode.id,
			testStartTime, [ "critical", reason ], testTimer); },
			SKIP: function(reason) { markHierarchyNode(thisNode.nodeType, thisNode.id, 
			testStartTime, [ "skip", reason ], testTimer); }
		});
	      
\end{verbatim}

Of course this loose way of passing functions is dangerous , we make sure that it is a valid function before  committing to database.
we do this on the client side because we assume we trust the developer will only make mistakes and not maliciously send to this 
protected endpoint (with HTTP Auth.)

you can put arbitrary html for the reason for test, but we prefer users stick to simple text to avoid any issues, 
The test framework is robust enough to, as long as the minimal structure of a test is sound, gracefully skip problematic entries in case of error.

You can see from the above text that you have the convenience of reusing any of the template functions that are already in place for other categories
this is very good news for extensibility in browseraudit. The only missing part is some way of dealing with browsing and adding html templates
to use along those functions, to which unfortunately we don't have a current way of doing but is an easy thing to add in the future and we
can already tinker manually to find these by browsing the server folder directories corresponding to tests like /sop/ajax.html (for example).

\subsubsection{User Interface}

the UI was made using knockoutjs \cite{knockoutjs} library a Model-View-View Model pattern, it has proven to be a very powerful framework that once mastered allowed
to avoid the trouble that dealing with complex HTML and events can become. It also has the advantage of supporting all browsers above ie6 which matches one
of the objectives behind version 2 of browseraudit that warranted a rewrite of the test framework.

The result is a pretty easy to use minimalistic interface. We decided to go for the main bootstrap theme for style to which we took some liberties
with the coloring and layout. The UI is presentable, readable and mobile friendly out of the box.

you can see a snapshot of the main interface in \ref{fig:addtest} as you can see , it first asks you to select a sub category from the side collapsible
menu which it clearly highlights in blue above the title input. The rest of the fields in the form are self-explanatory with a main function 
input code that was manually built but to which the lines are not used for anything at the moment but is reserved for future possible
linting of the javascript or indicating source of errors.

The function textarea by default shows the default function which is runnable to explain or remind the user of the format. 
Below it is a notice reminding users that they can fetch the test template function and example usage of this function by clicking the fetch
button. this for example gets the following data and presents it nicely with the help of SHJS code highlighting library , \cite{SHJS},customized with a suitable theme.

\ref{fig:function}


In order to avoid any accidents and make sure the user has double checked his code add test triggers a modal that asks for confirmation with the code
and category to be added to , see \ref{fig:confirm} 
\ref{fig:confirm}

If successful a success field shown in \ref{fig:success} appears, a similar widget appears for errors with the error message.

Once successful you can head to /test to see your test running properly, hooray!

\subsection{Adding new tests}

we start simple, because simple is beautiful. The simplest types of tests we could thing of required a new category: Client-only tests.
This category as we said before is a placeholder for tests that have their own custom function that don't fit anywhere else.

we are talking mainly about Javascript tests, postmessage on local frames, navigation in same window for example.

these tests are self-explanatory and allows us to gauge the external quality of our test adding interface progressively, meaning first only needing
to add client code to the database and registering the results of their execution on the client, bypassing the need to setup endpoints with particular types
of responses or other complex configuration. 

\subsubsection{Strict mode javascript tests}

The first test we're going to include deals with Strict Mode, it is a new feature in ECMAScript 5 that allows you to place a program, \
or a function, in a "strict" operating context. This strict context prevents certain actions from being taken and throws more exceptions.\
to use this feature suffice it to add the  "use strict"; keyword at the top of the page , or very conveniently in our case , in the context \
of a function only. Using strict mode allows to raise errors on using certain bad practices of ES3 which ES5 is normally fully retroactively compatible with.\

What is not allowed in strict mode, according to \cite{resig}:
\begin{itemize}
 \item use of implicit or undefined globals will result in error.
 \item Deleting a variable, a function, or an argument will result in an error, instead of silently failing.
 \item defining a property more than once in an object literal will cause an exception to be thrown.
 \item Virtually any attempt to use the name ‘eval’ is prohibited – as is the ability to assign the eval function to a variable or a property of an object.
 \item Additionally, attempts to introduce new variables through an eval will be blocked.
 \item with(){} is a syntax error.
\end{itemize}

Since all of these are restrictions, we will trust that it doesn't over restrict and will just test for negative properties for each of these prohibited behaviour.\

Hence, our first test is:

\begin{verbatim}
 function(){
  "use strict";
  try{
    bignametoavoidclash = 'initialising a global, -10000 marks!';
  }
  catch(bro){
    this.PASS("exception was raised, this is the desired behaviour");
    return;
  }
  this.FAIL("the exception was not thrown, ES5 strict mode globals are not properly implemented");
 }
\end{verbatim}

\subsubsection{other potential tests to be added}

Another type of test that is client side only that we will add is navigation policy. At the moment this is one of the things
that are unwritten law. our mechanism is powerful enough to be able to add one type of navigation policy test which are
within the same origin and window. so in plain english we could write a test that tests wether a frame is allowed
to navigate another frame within the same window somewhere but not what are called cross window attacks. 
see \cite{infosys} for details which itself sites the excellent stanford article \cite{stanford}.

This is a top priority at the time of writing and hopefuly will get to present it to the reader. it also nicely
shows the limitations in terms of not being able to create new html from the test adding interface for the cross-window navigation policy.
It also shows that it is not a fatality since we could use the /static folder to hold such template and fetch them from 
our code even if such fix is not ideal and error prone.

\subsection{statistics page}

Our tests test a plethora of browser features and neatly present the results to your browser in the /test page, but we felt that a lot of valuable information
was laying in the database that wouldn't get shown to the user, so we decided to aggregate user executions data in one interface available at /stats or again
by following the statistics link from the landing page.

This page shows the results across unique browsers (or User-Agent strings) taking the latest execution from that particular type of browser and logging the results
per subcategory per test with all the tallies for an execution, a subcategory a category and all categories cumulated and indicated clearly next to these partitions.

\subsubsection{User interface}

We chose a simple layout, reusing the look and feel of the category picker from the manage\_test page. It shows at the bottom the grand total across all categories
for all (browser) unique executions :number of passes/ total , number of criticals/total , number of warnings/total .

We then used badges on the list of categories to contain the tallies of the categories and sub categories which are collapsible and colour coded for convenience.

Clicking on a sub category reveals a full width table containing test results per unique browser execution. The outcome for that clearly shown and a last column
with the totals for a unique browser execution at the end that also follows the same format , namely: number of passes/ total , number of criticals/total , number of warnings/total.
see \ref{fig:stats}

\subsubsection{what it offers}
We found that after many redesigns this way of doing things gave users the most important information the most concisely. In fact just through debugging it
we found all the entries that were flagged as problematic in Charlie's paper \cite{charlie} quickly , this design was chosen because it enables finding 
problematic entries quickly and forces the user to think about the relevant categorization of data and we are content with what it offers.
Building this interface required a fair amount of plumbing on the server and also in terms of knockout and view code which is something we made future proof
meaning that beyond what it offers the statistics page is a stepping stone towards other uses of that data and as you can see we have also modularised our
category "processing" code and reused it already in the statistics page and we believe that much of this will be reused in our third and final direction in which we
chose to extend Browseraudit which will be discussed next.

\section{Open ended Goal: Data-driven browser security reference for the developer }
\label{open}

The next natural thing our research has brought us to is what we called a Data-driven browser security reference a clumsy name perhaps but we haven't found an equivalent
and once built it should be self-explanatory. The idea stemmed from the simple enquiry , since Browseraudit's audience are mainly developers and security researchers/enthusiasts
we asked ourselves what would be the most useful thing I can get from all the data we have in Browseraudit and our domain knowledge we have about browser client security.
The best thing we could thing of is what you can see in \ref{fig:mockup}.

Here's how it would work. You have a bunch of drop downs corresponding to different browsers you select. when you select an entry that looks like :
Ie6 it gets added to the list of constraints , which for example could have a [Firefox 38.0 x] already in and would become 
[[Ie6 x] , [Firefox 38.0]]. What it essentially means is the set of browsers you intent to support in any given web application you plan to deploy.

Next is where the 'magic' is. using our data we construct the set of features that working properly for those sets of browsers and query
a Datalog backend to use logical inference to tell us what security fixes to use for those browsers.

Admitedly this is still in the research and design stage. but to give you a rough idea of what we mean. The backend would be encoding or formalising
rules like :

\begin{verbatim}
 
Safe:- XSSSafe AND CSRFSafe AND SSLSafe AND ClickJackingSafe.

CSPGood(X):- (CSPPolicy(BLA) AND CSPWorks(BLA)
XSSSafe :- CSPWorks(X) OR (XSSFilter) OR Sandbox(BLA)).
CSRFSafe:- (CSPWorks(X) OR CSRFTokens).
ClickJackingSafe:- ((X-FrameOptions(BLA) AND hasXframeOptions(BLA)) OR TopCheckingCode).
SSLSafe :- HSTSGood AND GoodSSL AND.
XSSMitigate:- CookieSecure AND (CookieHTTPOnly).

\end{verbatim}

Since they are all positive logical properties and at the suggestion of our supervisor, the domain knowlege should be made into positive propositional logical statements we can
use datalog and avoid problems with negation. Although less elegant, our domain of tests propositions is finite and relatively small, we believe that we 
can avoid First Order Logic which is undecidable and is not needed but this has yet to be seen in the wild.

The idea is to simulate abductive logic programming by passing such a normal logic program a set of ground truth like {CSPWorks('object') , HSTSGood, CSRFWorks('bla')..}
which, as you can see is the reason why we said we wanted the set of things that work properly for those browsers from our frontend and browseraudit data.

Our normal logic program is written in a manner whose pattern is crudely:
SatisfyourStandardsofSecurity := (having all these conditions) OR (doingthisInstead) OR taking mitigation measure X).

because it is the most natural way we could thing of , but could have been written in the form of:
DoThis := (SatisfyourStandardsofSecurity).
DothisInstead := ifYouHavethisFallbackRequirement.
TakeMitigation := true. <- avoids using NOT but is also somewhat erroneous because it always takes mitigation.

We prefered our approach because it is more natural and allows for more expressivity and feels less 'hacky' do not worry if the details of
abductive logic programming is not fresh in your mind, what we mean here is collecting the trace of the datalog query. or in other words the ground truth
that makes the Safe predicate true. And convert it through a fixed mapping to practical text detailing what you should do as a web developer to secure your site
against client side attacks for those browsers.

We aim to have something to demo for this but the space of formalising the rules is very vast . It is also the most intellectually stimuating 
and if done right a huge benefit for both the standardisation community and developers at large.
